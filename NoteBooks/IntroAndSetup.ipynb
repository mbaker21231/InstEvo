{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution of Culture and Institutions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to a project that I have been working on, on and off, for some time. The basic idea is to use information on language and geography to supplement cross-cultural data sets such as the George Murdock's [**Ethnographic Atlas**](http://intersci.ss.uci.edu/wiki/index.php/Ethnographic_Atlas). The project involves creating tools for recreating migratory histories (really distributions of histories), including some of Python's wonderful geography and geometry packages, and then matching it with Ethnographic data. \n",
    "\n",
    "In the end, the hope is that I can get a feel for the nature of cultural evolution, and the mutual dependencies between culture, environment, and technological progress. \n",
    "\n",
    "## Historical Recreation\n",
    "\n",
    "The first step in the project is to recreate histories (or really distributions over possible migratory histories, based on what is known, and what seems most likely). Accordingly, in this initial notebook, I develop some Python tools for estimating branching times and locations. This approach is borrowed wholly from the historical linguistics literature, with a few tweaks to make it work better with available data. \n",
    "\n",
    "## Packages and Setup\n",
    "\n",
    "I have created (rather haphazardly as the experimental process is ongoing) two modules: `PyIEClasses` and `PyIETools` for which I hope to cook up thorough documentation, as the way that I have conceptualized and organized phylogenetic relationships is a little different than is usual I think. Anyways, here, we import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Some Kung-Fu needed to go to the next directory and import everything...\n",
    "\n",
    "start_dir = os.getcwd()\n",
    "python_dir = pathlib.Path(os.getcwd()).parts[:-1] + ('Python',)\n",
    "os.chdir(pathlib.Path(*python_dir))\n",
    "\n",
    "import PyInstEvo\n",
    "\n",
    "os.chdir(start_dir)\n",
    "os.chdir('..')\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "np.set_printoptions(linewidth=120)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Arranging Basic Data\n",
    "\n",
    "The data set consists of so=called [**Swadesh lists**](https://en.wikipedia.org/wiki/Swadesh_list) for approximately 4500 languages, which I have obtained from the [**Automatic Similarity Judgement Program**]( http://asjp.clld.org/). I have taken this data and merged it with the **Ethnographic Atlas** to the best of my ability. I have also merged in some information on climate and geography. Most of this was done in **`Stata`**, but might be improved on now that I have a better handle on Python. \n",
    "\n",
    "I have also, to the best of my ability, added in [**Merritt Ruhlen's**](https://en.wikipedia.org/wiki/Merritt_Ruhlen) linguistic classifications from his **Languages of the World**. I did this only because, as a [**Joseph Greenberg**](https://en.wikipedia.org/wiki/Joseph_Greenberg) enthusiast, Ruhlen likes reduction so his classification system made the number of different language stocks to deal with much smaller. In retrospect, nothing really depends upon this and I might be injecting controversy into the project by using these classifications, even if my purpose was pragmatic. \n",
    "\n",
    "The raw data that I use in the project should all be in the `\\\\IEData` folder in this repository. The source of my **Ethnographic Atlas** data is the [**World Cultures Journal**](http://www.worldcultures.org/).  \n",
    "\n",
    "One last note: it is interesting to note that Joseph Greenberg is a New Yorker from Brooklyn, and [**Morris Swadesh**](https://en.wikipedia.org/wiki/Morris_Swadesh) was employed by the City University of New York, like myself, up until he was fired in 1949 during the Red Scare for \"being a communist.\"\n",
    "\n",
    "Anyways...a first step is reading the data in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_stata(os.getcwd() + '\\\\IEData\\\\words_4_useMerged.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'trimFlag', 'ruhlen_1', 'ruhlen_2', 'ruhlen_3', 'lat1', 'lon1', 'eaName', 'wordCount', 'trimFlag2',\n",
      "       ...\n",
      "       'v99', 'vclimate1', 'vclimate2', 'v1000', 'vmet', 'vweave', 'vpot', 'vtechcomp', 'langnotes', '_merge'], dtype='object', length=268)\n"
     ]
    }
   ],
   "source": [
    "# A look at all the columns\n",
    "\n",
    "print(Data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the variables `ruhlen_1`, `ruhlen_2`, etc. appear. These variables have a nested panel structure, so in the code below, I create string variables that get progressively more unique so I have a result that can be rendered as nested panels. In due time: for now, a first thing to have might be a means of adding in additional information about the nesting structure. \n",
    "\n",
    "What follows is an example where I shift all the information to the right for a particular language stock (Altaic) and then fill in a new base grouping, which keeps together Japanese and Korean languages as a first branching. Note that there are [two levels of controversy here](https://en.wikipedia.org/wiki/Altaic_languages) - whether or not Japanese and Korean should be together, and also whether or not they should be included in Altaic at all. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjbaker\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(2, 16)):\n",
    "    to  = 'ruhlen_' + str(i + 1)\n",
    "    fro = 'ruhlen_' + str(i)\n",
    "    Data.loc[ Data['ruhlen_1'] == 'ALTAIC', to] = Data.loc[ Data['ruhlen_1'] == 'ALTAIC', fro]\n",
    "\n",
    "\n",
    "ruhlenTree=['ruhlen_1', 'ruhlen_2', 'ruhlen_3', 'ruhlen_4', 'ruhlen_5', 'ruhlen_6', 'ruhlen_7', 'ruhlen_8',\n",
    "                'ruhlen_9', 'ruhlen_10', 'ruhlen_11', 'ruhlen_12', 'ruhlen_13', 'ruhlen_14', 'ruhlen_15', 'ruhlen_16']    \n",
    "    \n",
    "Data[ruhlenTree].loc[Data['ruhlen_1'] == 'ALTAIC']\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_1'] == 'ALTAIC'] = 'AltaicProp'\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_3'] == '']       = 'JaponicKorean'\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_3'] == 'Japanese-Ryukyuan'] = 'JaponicKorean'\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_3'] == 'Ryukyuan']          = 'JaponicKorean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get down to details and sort the data according to the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.sort_values(by=ruhlenTree, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One aspect of the project is that 4500 language groups have proven to be too many for me to deal with all at once! I therefore created a variable called `trimFlag` which I use to trim down the list. I have made some modifications to this file at various times for reasons I forget, but these are all catalogued in the file `TrimMod.txt`. To read in this file and arrange it isn't so bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimList = []\n",
    "try:\n",
    "    trimFile = open(os.getcwd() + '//IEData//TrimMod.txt', 'r')\n",
    "    for line in trimFile.readlines():\n",
    "        trimList.append(line.replace('\\\"', '').replace('\\\\', '').rstrip())\n",
    "except:\n",
    "    print('Wrong computer!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages in the above list we wish to keep because they are important. I'm envisioning that we will want to change it at some point, so its useful to have it in the above form, where we can add or subtract languages without too much problem. The variable `trimFlag` we use to mark with a one languages we want to exclude, hence we keep all the languages with a `trimFlag` of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMERIND      205\n",
      "INDOPAC      126\n",
      "NIGERKORD    110\n",
      "AUSTRIC      105\n",
      "INDOHITT      96\n",
      "NILOSAHAR     93\n",
      "SINOTIBET     86\n",
      "AUSTRAL       84\n",
      "AFROASIA      77\n",
      "ALTAIC        39\n",
      "CAUCASIAN     24\n",
      "URALICYUK     23\n",
      "NADENE        21\n",
      "ELAMODRA      17\n",
      "KHOISAN       17\n",
      "BURUSHASK     10\n",
      "ESKIMOAL       9\n",
      "KHET           6\n",
      "CHUKCHI        5\n",
      "NAHALI         2\n",
      "BASQUE         1\n",
      "SUMERIAN       1\n",
      "GILYAK         1\n",
      "Name: ruhlen_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Data.loc[Data.name.isin(trimList), 'trimFlag'] = 0\n",
    "Data = Data.loc[Data['trimFlag'] == 0]\n",
    "\n",
    "print(Data['ruhlen_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get rid of a few pigeon languages and things of that nature. Moreover, we need to clean up some of the expiry dates as languages that have effectively existed until the present don't need to be seen as moribund (this adds parameters to the modeling below. \n",
    "\n",
    "For now, we also suppose that the standard deviation of expiration dates is 40 (rather arbitrarily - but this really means it is most likely plus or minus a century). \n",
    "\n",
    "Line by line, I:\n",
    "- drop pidgeon languages\n",
    "- drop languages that don't have categories\n",
    "- If a language died to recently, make it non-expired\n",
    "- Fill in a value for the standard deviation of the expiration date\n",
    "- If a language is missing a name, fill in the Ethnographic Atlas name\n",
    "- Make a container for a dead language dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.loc[Data.ruhlen_3.str.contains('based') == False] \n",
    "\n",
    "Data['ruhlen_1'].replace('', np.nan, inplace=True)              \n",
    "\n",
    "Data.dropna(subset = ['ruhlen_1'], inplace=True)                 \n",
    "\n",
    "Data.loc[Data['ex_date'] > 1900, 'ex_date'] = np.NaN             # missing values\n",
    "\n",
    "Data['ex_date_sd'] = np.NaN\n",
    "\n",
    "Data.loc[Data['ex_date'] != np.NaN, 'ex_date_sd'] = 40\n",
    "\n",
    "Data.loc[Data['name'] == '','name'] = Data['eaName']\n",
    "\n",
    "Data['dead'] = 0\n",
    "Data['deadOne'] = 1-Data['dead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick look at the data, to be sure that it is sorted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ruhlen_1 ruhlen_2\n",
      "104   AFROASIA   Berber\n",
      "310   AFROASIA   Berber\n",
      "3867  AFROASIA   Berber\n",
      "207   AFROASIA   Berber\n",
      "206   AFROASIA   Berber\n",
      "1002  AFROASIA   Berber\n",
      "106   AFROASIA   Berber\n",
      "601   AFROASIA   Berber\n",
      "3172  AFROASIA   Berber\n",
      "603   AFROASIA   Berber\n",
      "    ruhlen_1       ruhlen_2   ruhlen_3\n",
      "43   AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "136  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "405  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "699  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "137  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "342  AUSTRIC  Austroasiatic      Munda\n",
      "28   AUSTRIC  Austroasiatic      Munda\n",
      "317  AUSTRIC  Austroasiatic   Paiwanic\n",
      "689  AUSTRIC  Austroasiatic   Paiwanic\n",
      "117  AUSTRIC       Miao-Yao     Mienic\n"
     ]
    }
   ],
   "source": [
    "print(Data[ ['ruhlen_1', 'ruhlen_2'] ][0:10])\n",
    "print(Data[ ['ruhlen_1', 'ruhlen_2', 'ruhlen_3'] ][500:510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested panels from classifications\n",
    "\n",
    "\n",
    "We want to render the data in the form of nested panels. So, in the above, we would have the *Afroasia* group marked by a unique dummy variable, and then have the *Afroasia / Berber* group have its own dummy, etc. The follow is one way to create unique strings for each nested group, and then to transform them into numbers. \n",
    "\n",
    "This is done in the following block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['T1'] = Data.ruhlen_1\n",
    "for x in range(2, 17):\n",
    "    T       = 'T' + str(x)\n",
    "    Tm1     = 'T' + str(x - 1)\n",
    "    ruhlen  = 'ruhlen_' + str(x)\n",
    "    Data[T] = Data[Tm1] + Data[ruhlen]\n",
    "Data['T17'] = Data.T16+Data.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is a way to arrange all these grouping variables into what is something like a Stata local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns  = []\n",
    "columns2 = []\n",
    "for x in range(1, 17):\n",
    "    columns.append('ruhlen_' + str(x))\n",
    "    columns2.append('T' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks okay, but let's check and see that we have all unique identifiers at the end.\n",
    "That is, our last column should uniquely identify each language with a number. Just as a check, let's be sure that everything in the last row is unique!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(Data) == len(set(Data['T17'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next job is to make unique numerical identifiers for each thing just so they are easier to look at, \n",
    "if for no other reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 "
     ]
    }
   ],
   "source": [
    "for n in range(1, 18):\n",
    "    print(n, end= ' ')\n",
    "    counter = 1\n",
    "    TR       = 'TR' + str(n)\n",
    "    T        = 'T' + str(n)\n",
    "    Data[TR] = np.nan\n",
    "    for x in collections.OrderedDict.fromkeys(Data[T]):   \n",
    "        Data.loc[Data[T] == x, TR] = counter\n",
    "        counter = counter + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a lot of words, but many are missing, we can pare things down to a reduced Swadesh list of those words that are most commonly occuring in the data. These are the following. Once we have it up and running we can take a quick look at a few words. One thing we should also add is some description of what these words are!\n",
    "\n",
    "At some point, we should add in word meanings. But let's get our pared-down list in place, and then take a look at how the **ASJP** renders them in `Ascii`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   name word1 word2      word3 word11     word18   word77\n",
      "104                                SIWI   niS   Sik     inCini   ijin    bunadem    adxax\n",
      "310               TASHELHIT/IDA_USEMLAL    nk    ky       nkni    yan      bnadm   tagunt\n",
      "3867  TAMAZIGHT_CENTRAL_ATLAS/AYT_NDHIR   n3k   S3g      nukni    yun    b3na83m    azr7u\n",
      "207                             METMATA  n3CC  S3kk      n3Sni                       azru\n",
      "206                             TUMZABT   n3S   n3C     n3Snin   ig3n     bnad3m     adxa\n",
      "1002                     OUARGLA_BERBER  n3SS  S3kk     n3Snin  igg3n    takrumt    adXaX\n",
      "106               TARIFIT/BENI_IZNASSEN   n3C   S3k      n3Cin   ij3n     bna83m     azru\n",
      "601                      AHAGGAR_TUAREG   nak   kay  nakkaned7   iyan     awadam   tahunt\n",
      "3172                           TAMASHEQ   nak   kay   nakaned7   iyan     awad3m   t3hunt\n",
      "603                              ZENAGA  n37k   k3k      n3kni   yu7n  3g8inad3m  t37rg3t\n"
     ]
    }
   ],
   "source": [
    "words=['word1', 'word2', 'word3', 'word11', 'word12', 'word18', 'word19', 'word21', 'word22', 'word23', \n",
    "       'word25', 'word28', 'word30', 'word31', 'word34', 'word39', 'word40', 'word41', 'word43', 'word44', \n",
    "       'word47', 'word48', 'word51', 'word53', 'word54', 'word57', 'word58', 'word61', 'word66', 'word72', \n",
    "       'word74', 'word75', 'word77', 'word82', 'word85', 'word86', 'word92', 'word95', 'word96', 'word100']\n",
    "print(Data[ ['name', 'word1', 'word2','word3','word11','word18', 'word77'] ][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loc = np.where(Data['name'] == 'ENGLISH')[0][0]\n",
    "\n",
    "EnglishList=[]\n",
    "for word in words:\n",
    "    EnglishList.append(Data[word].iloc[Loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ei yu wi w3n tu pers3n fiS dag laus tri lif skin bl3d bon horn ir Ei nos tu8 t3N ni hEnd brest liv3r drink si hir dEi k3m s3n star wat3r ston fEir pE8 maunt3n nEit ful nu nem "
     ]
    }
   ],
   "source": [
    "for word in EnglishList:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can intuit, anyways, that the words are: I, You, We, One, Two, Person (man), Fish, Dog, Louse, Tree, Leaf, Skin, Blood, Bone, Horn, Ear, Eye, Nose, Tooth, Tongue, Knee, Hand, Breast, Liver, Drink, See, Hear, Die, Come, Sun, Star, Water, Stone, Fire, Path, Mountain, Night, Full, New, Name. If one is interested, one can read about how and why these words are chosen from the [Swadesh list Wikipedia entry](https://en.wikipedia.org/wiki/Swadesh_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dogolpolsky classes\n",
    "\n",
    "The first actual function we will make reduces words into so-called Dogopolsky classes. These classes, while simple and inexact, have a nice attribute from the perspective of comparative linguistics: a constant number of states. If one relies on expert cognancy judgements, as is commonly done in historical linguistics, we have more classes for larger linguistic stocks. We avoid this by using a constant set of 10 states for languages. Some research suggests this isn't a terrible first approximation. \n",
    "\n",
    "We have two functions in the `PyIETools` module that accomplish this: `worddolg` which takes a word and returns its Dogolopolsky class, and `worddolgs`, which does the same for a list of words. We apply these functions and get back numeric classes in the following block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the function, we can convert our list into an array of Dogopolsky classes. We also print out a sample of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 2. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Words = np.asarray(Data[words])\n",
    "W = PyInstEvo.worddolgs(Words[:][0])\n",
    "\n",
    "for i in range(1, len(Words)):\n",
    "    reduction = PyInstEvo.worddolgs(Words[:][i])\n",
    "    W = np.vstack((W,reduction))\n",
    "print(W[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print statement shows what we have - an array of numbers for each society/language telling us in which class each word on our list belongs to. \n",
    "\n",
    "\n",
    "In estimation, we need to have these in a dummy variable form. The function `charnum_to_dummies` translates the numbers from the above work into a string of 10 dummies. Each word then becomes amenable to Markov chain analysis. Class 3, for example,  becomes\n",
    "`[0 0 1 0 0 0 0 0 0 0]`. Felsenstein notes that this can easily handle missing words: we just use a state indicator vector with all ones in it. `[1 1 1 1 1 1 1 1 1 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word10_p', 'word10_t', 'word10_s', 'word10_c', 'word10_m']\n",
      "104     0\n",
      "310     0\n",
      "3867    0\n",
      "207     0\n",
      "206     0\n",
      "1002    0\n",
      "106     0\n",
      "601     0\n",
      "3172    0\n",
      "603     0\n",
      "Name: word10_p, dtype: uint8\n"
     ]
    }
   ],
   "source": [
    "States = PyInstEvo.charnum_to_dummies(W)\n",
    "\n",
    "states = []\n",
    "dim1   = int(np.shape(States)[1]/10)\n",
    "DogList= ['p', 't', 's', 'c', 'm', 'N', 'l', 'w', 'y', 'i']\n",
    "for i in range(0, dim1):\n",
    "    for j in range(0, 10):\n",
    "        states.append(words[i] + str(i) + '_' + DogList[j])\n",
    "\n",
    "print(states[0:5])\n",
    "\n",
    "for i in range(0, int(dim1*10)):\n",
    "    Data[ states[i] ] = States[:, i]\n",
    "\n",
    "# Print out result for verification:\n",
    "\n",
    "print(Data['word10_p'][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small sample. If a word is missing, all of its states are possible so it is a bunch of ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "States[:5,0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prior information on language splits\n",
    "\n",
    "The next thing to be done is to read in all of our prior information on splits. This is fundamental in manipulating trees, as it is what one must use to calibrate the tree. \n",
    "\n",
    "We have gathered this information from various sources which we should indicate, including Ackerman, the ASJP website, and also some additional sources. In fact, what we need to do is write in another column, so we keep track of where the sources are! Of course, the methods we use can easily allow for additional prior information on linguistic splits. \n",
    "\n",
    "All of our information is expressed in terms of bilateral splits. We read this in as a separate data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Splits=pd.read_csv(os.getcwd() + '//IEData//PriorSplitInfo.csv',\n",
    "                   names=('phylum','language1','language2','years','sdyears'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       phylum        language1                language2  years  sdyears\n",
      "0   AfroAsia          ETHIOPIC                     GEEZ   2450       10\n",
      "1   AfroAsia          ETHIOPIC                    TIGRE   2450       10\n",
      "2   AfroAsia          ETHIOPIC                  ARGOBRA   2450       10\n",
      "3   AfroAsia           MALTESE  TUNISIAN_ARABIC_MAGHRIB    910       10\n",
      "4   AfroAsia     EASTERN_OROMO              MECHA_OROMO   2500       10\n",
      "5   AfroAsia     EASTERN_OROMO                  W_OROMO    460       10\n",
      "6   AfroAsia           W_OROMO                     ORMA    460       10\n",
      "7     Altaic         MONGOLIAN                   MOGHOL    750       10\n",
      "8     Altaic           TURKISH           JONK_KHORASANI   1419       10\n",
      "9     Altaic            MANCHU                    HEZHE    236       10\n",
      "10    Altaic            KYRGYZ              KAZAN_TATAR    900       10\n",
      "11    Altaic           CHUVASH                    UZBEK   2500       10\n",
      "12    Altaic            KOREAN                 JAPANESE   5000      200\n",
      "13    Altaic         AINU_SARU                   KOREAN   5000      300\n",
      "14    Altaic   BURIAT_MONGOLIA                   MOGHOL    750       10\n"
     ]
    }
   ],
   "source": [
    "print(Splits[0:15])            # A small sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The other source of prior information for calibration of linguistic trees is prior depth. We haven't worked nearly hard enough on setting these up, being content merely to figure out how to incorporate the information for the time being. We have to go through this and pin down some information that is a bit more exact for the approximate depth of each language stock (in terms of millenia from the present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       phylum  min  max\n",
      "0   AfroAsia     9   12\n",
      "1     Altaic     6    8\n",
      "2     Amerind    9   11\n",
      "3     Austric    8   12\n",
      "4     Austral    9   12\n",
      "5   Burushask    3    6\n",
      "6   Caucasian    4    7\n",
      "7     Chukchi    3    6\n",
      "8    ElamoDra    4    6\n",
      "9    EskimoAl    4    5\n",
      "10   IndoHitt    7    9\n",
      "11    IndoPac    8   10\n",
      "12       Khet    3    5\n",
      "13    Khoisan   10   12\n",
      "14     NaDene    6    8\n",
      "15  NigerKord   10   12\n",
      "16  NiloSahar   10   12\n",
      "17  SinoTibet    6    8\n",
      "18  UralicYuk    5    7\n"
     ]
    }
   ],
   "source": [
    "Depths=pd.read_csv(os.getcwd() + '//IEData//PriorDepth.csv', \n",
    "                   names=('phylum','min','max'))\n",
    "print(Depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final thing to do, we need to organize our material on expiration dates of certain languages. Some of this information is in the original data, while some of it is in a free-standing file, so we want to merge this information together and into our original data file. \n",
    "\n",
    "Here goes, while reading from the `DeathMods.csv` file which describes modifications to moribund languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DeathDates = pd.read_csv(os.getcwd() + '//IEData//DeathMods.csv', \n",
    "                         names=('language', 'ex_date', 'ex_date_sd'))\n",
    "DeathDates.rename(columns = {'language': 'name'}, inplace=True)\n",
    "Data.set_index('name')\n",
    "Data['ex_date_new']    = pd.Series(DeathDates['ex_date'], index=DeathDates.name)\n",
    "Data['ex_date_sd_new'] = pd.Series(DeathDates['ex_date_sd'], index=DeathDates.name)\n",
    "\n",
    "# Now, replace ex_date with ex_date_new if ex_date_new is not missing...\n",
    "\n",
    "Data.loc[Data['ex_date_new']<10000 , \"ex_date\"  ]    = Data['ex_date_new']\n",
    "Data.loc[Data['ex_date_sd_new']<10000, \"ex_date_sd\"] = Data['ex_date_sd_new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling\n",
    "\n",
    "As a last step, we will pickle the data for ease of access in our next workbook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.to_pickle(os.getcwd()    + '//IEData//MasterData.pkl')\n",
    "Splits.to_pickle(os.getcwd() + '//IEData//Splits.pkl')\n",
    "Depths.to_pickle(os.getcwd()  + '//IEData//Depths.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Subsequent work builds on this combined data through development of tools and classes. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
