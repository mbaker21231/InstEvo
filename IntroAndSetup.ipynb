{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution of Culture and Institutions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a project that I have been working on for some time. The basic idea is to use information on language and geography to supplement cross-cultural data sets such as the George Murdock's [**Ethnographic Atlas**](http://intersci.ss.uci.edu/wiki/index.php/Ethnographic_Atlas). The project involves creating tools for recreating migratory histories, and then matching it with data. \n",
    "\n",
    "## Historical Recreation\n",
    "\n",
    "The first step in the project is to recreate histories (or really distributions over possible migratory histories, based on what we know and what seems most likely). Accordingly, in this initial notebook, I develop some Python tools for estimating branching times and locations; the idea is to modify existing literature on glottochronology to suit the needs of my larger project (which involves the study of cultural and institutional evolution over time). \n",
    "\n",
    "## Packages and Setup\n",
    "\n",
    "The following packages are required. Additionally, I have created (rather haphazardly as the experimental process is ongoing) two modules: `PyIEClasses` and `PyIETools` for which I hope to cook up thorough documentation, as the way that I have conceptualized and organized phylogenetic relationships is a little different than is usual I think. Anyways, here, we import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "# My two packages\n",
    "\n",
    "import PyIETools\n",
    "import PyIEClasses\n",
    "\n",
    "# Set some parameters:\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "np.set_printoptions(linewidth=120)\n",
    "\n",
    "# Use some magics for ease of reading:\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and Arranging Basic Data\n",
    "\n",
    "The data set consists of Swadesh lists for approximately 4500 languages, which I have obtained from the [**Automatic Similarity Judgement Program**]( http://asjp.clld.org/). I have taken this data and merged it with the **Ethnographic Atlas** to the best of my ability, and I have also merged in some information on climate and information. Most of this was done in **`Stata`**, and I hope to post some details on how all this works as well. \n",
    "\n",
    "I have also, to the best of my ability, added in Merritt Ruhlen's linguistic classifications from his **Languages of the World**. I did this only because, as a Greenberg enthusiast, Ruhlen likes reduction so his classification system made the number of different language stocks to deal with much smaller. In retrospect, nothing really depends upon this and I might be injecting controversy into the project by using these classifications, even if my purpose was purely pragmatic. \n",
    "\n",
    "The raw data that I use in the project should all be in the `\\\\IEData` folder in this repository. The source of my **Ethnographic Atlas** data is the **World Cultures Journal**. I will eventually post links to all of these sources. \n",
    "\n",
    "Anyways...a first step is reading the data in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the data in\n",
    "\n",
    "Data = pd.read_stata('IEData\\\\words_4_useMerged.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'trimFlag', 'ruhlen_1', 'ruhlen_2', 'ruhlen_3', 'lat1', 'lon1', 'eaName', 'wordCount', 'trimFlag2',\n",
      "       ...\n",
      "       'v99', 'vclimate1', 'vclimate2', 'v1000', 'vmet', 'vweave', 'vpot', 'vtechcomp', 'langnotes', '_merge'], dtype='object', length=268)\n"
     ]
    }
   ],
   "source": [
    "# A look at all the columns\n",
    "\n",
    "print(Data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the variables `ruhlen_1`, `ruhlen_2`, etc. appear. These variables have a nested panel structure, so in the code below, I create string variables that get progressively more unique so we get something that can be rendered as nested panels. In due time: for now, a first thing to have might be a means of adding in additional information about the nesting structure. \n",
    "\n",
    "What follows is an example where I shift all the information to the right for a particular language stock (Altaic) and then fill in a new base grouping, which keeps together\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjbaker\\AppData\\Local\\Continuum\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# We probably want to know how to add in groups. I think we can safely do the following:\n",
    "# For the Altaic languages, shift all the Ruhlen variables to the right:\n",
    "\n",
    "for i in reversed(range(2, 16)):\n",
    "    to  = 'ruhlen_' + str(i + 1)\n",
    "    fro = 'ruhlen_' + str(i)\n",
    "    Data.loc[ Data['ruhlen_1'] == 'ALTAIC', to] = Data.loc[ Data['ruhlen_1'] == 'ALTAIC', fro]\n",
    "\n",
    "\n",
    "ruhlenTree=['ruhlen_1', 'ruhlen_2', 'ruhlen_3', 'ruhlen_4', 'ruhlen_5', 'ruhlen_6', 'ruhlen_7', 'ruhlen_8',\n",
    "                'ruhlen_9', 'ruhlen_10', 'ruhlen_11', 'ruhlen_12', 'ruhlen_13', 'ruhlen_14', 'ruhlen_15', 'ruhlen_16']    \n",
    "    \n",
    "Data[ruhlenTree].loc[Data['ruhlen_1'] == 'ALTAIC']\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_1'] == 'ALTAIC'] = 'AltaicProp'\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_3'] == '']       = 'JaponicKorean'\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_3'] == 'Japanese-Ryukyuan'] = 'JaponicKorean'\n",
    "Data['ruhlen_2'].loc[Data['ruhlen_3'] == 'Ryukyuan']          = 'JaponicKorean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get down to details and sort the data according to the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data.sort_values(by=ruhlenTree, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One aspect of the project is that 4500 language groups have proven to be too many for me to deal with all at once! I therefore created a variable called `trimFlag` which I use to trim down the list. I have made some modifications to this file at various times for reasons I forget, but these are all catalogued in the file `TrimMod.txt`. To read in this file and arrange it isn't so bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in languages we wish to change the trim flag for and make a list\n",
    "\n",
    "trimList = []\n",
    "trimFile = open('IEData\\\\TrimMod.txt', 'r')\n",
    "for line in trimFile.readlines():\n",
    "    trimList.append(line.replace('\\\"', '').replace('\\\\', '').rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The languages in the above list we wish to keep because they are important. I'm envisioning that we will want to change it at some point, so its useful to have it in the above form, where we can add or subtract languages without too much problem. The variable `trimFlag` we use to mark with a one languages we want to exclude, hence we keep all the languages with a `trimFlag` of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMERIND      205\n",
      "INDOPAC      126\n",
      "NIGERKORD    110\n",
      "AUSTRIC      105\n",
      "INDOHITT      96\n",
      "NILOSAHAR     93\n",
      "SINOTIBET     86\n",
      "AUSTRAL       84\n",
      "AFROASIA      77\n",
      "ALTAIC        39\n",
      "CAUCASIAN     24\n",
      "URALICYUK     23\n",
      "NADENE        21\n",
      "KHOISAN       17\n",
      "ELAMODRA      17\n",
      "BURUSHASK     10\n",
      "ESKIMOAL       9\n",
      "KHET           6\n",
      "CHUKCHI        5\n",
      "NAHALI         2\n",
      "GILYAK         1\n",
      "SUMERIAN       1\n",
      "BASQUE         1\n",
      "Name: ruhlen_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Data.loc[Data.name.isin(trimList), 'trimFlag'] = 0\n",
    "Data = Data.loc[Data['trimFlag'] == 0]\n",
    "\n",
    "# Printout of the prevalence of languages from each linguistic stock, after removal of some languages\n",
    "\n",
    "print(Data['ruhlen_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to get rid of a few pigeon languages and things of that nature. Moreover, we need to clean up some of the expiry dates as languages that have effectively existed until the present don't need to be seen as moribund (this adds parameters to the modeling below. \n",
    "\n",
    "For now, we also suppose that the standard deviation of expiration dates is 40 (rather arbitrarily - but this really means it is most likely plus or minus a century). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A few basic cleaning operations - dropping the pigeon languages  \n",
    "\n",
    "Data = Data.loc[Data.ruhlen_3.str.contains('based') == False] \n",
    "\n",
    "# If no categorical information, drop \n",
    "\n",
    "Data['ruhlen_1'].replace('', np.nan, inplace=True)              \n",
    "Data.dropna(subset = ['ruhlen_1'], inplace=True)                 \n",
    "\n",
    "# Rework moribund languages\n",
    "\n",
    "Data.loc[Data['ex_date'] > 1900, 'ex_date'] = np.NaN             # missing values\n",
    "Data['ex_date_sd'] = np.NaN\n",
    "\n",
    "# Fill in the standard deviation of the expiry date\n",
    "\n",
    "Data.loc[Data['ex_date'] != np.NaN, 'ex_date_sd'] = 40\n",
    "Data.loc[Data['name'] == '','name'] = Data['eaName']\n",
    "\n",
    "# Some stuff to create a dummy variable marking moribund languages - dead is a little harsh but is a short handle...\n",
    "\n",
    "Data['dead'] = 0\n",
    "Data['deadOne'] = 1-Data['dead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick look at the data, to be sure that it is sorted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ruhlen_1 ruhlen_2\n",
      "104   AFROASIA   Berber\n",
      "310   AFROASIA   Berber\n",
      "3867  AFROASIA   Berber\n",
      "207   AFROASIA   Berber\n",
      "206   AFROASIA   Berber\n",
      "1002  AFROASIA   Berber\n",
      "106   AFROASIA   Berber\n",
      "601   AFROASIA   Berber\n",
      "3172  AFROASIA   Berber\n",
      "603   AFROASIA   Berber\n",
      "    ruhlen_1       ruhlen_2   ruhlen_3\n",
      "43   AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "136  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "405  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "699  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "137  AUSTRIC  Austroasiatic  Mon-Khmer\n",
      "342  AUSTRIC  Austroasiatic      Munda\n",
      "28   AUSTRIC  Austroasiatic      Munda\n",
      "317  AUSTRIC  Austroasiatic   Paiwanic\n",
      "689  AUSTRIC  Austroasiatic   Paiwanic\n",
      "117  AUSTRIC       Miao-Yao     Mienic\n"
     ]
    }
   ],
   "source": [
    "print(Data[ ['ruhlen_1', 'ruhlen_2'] ][0:10])\n",
    "print(Data[ ['ruhlen_1', 'ruhlen_2', 'ruhlen_3'] ][500:510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested panels from classifications\n",
    "\n",
    "\n",
    "We want to render the data in the form of nested panels. So, in the above, we would have the *Afroasia* group marked by a unique dummy variable, and then have the *Afroasia / Berber* group have its own dummy, etc. The follow is one way to create unique strings for each nested group, and then to transform them into numbers. \n",
    "\n",
    "This is done in the following block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Data['T1'] = Data.ruhlen_1\n",
    "for x in range(2, 17):\n",
    "    T       = 'T' + str(x)\n",
    "    Tm1     = 'T' + str(x - 1)\n",
    "    ruhlen  = 'ruhlen_' + str(x)\n",
    "    Data[T] = Data[Tm1] + Data[ruhlen]\n",
    "Data['T17'] = Data.T16+Data.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is a way to arrange all these grouping variables into what is something like a Stata local.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns  = []\n",
    "columns2 = []\n",
    "for x in range(1, 17):\n",
    "    columns.append('ruhlen_' + str(x))\n",
    "    columns2.append('T' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything looks okay, but let's check and see that we have all unique identifiers at the end.\n",
    "# That is, our last column should uniquely identify each language with a number\n",
    "\n",
    "len(Data) == len(set(Data['T17']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The next job is to make unique numerical identifiers for each thing just so they are easier to look at, \n",
    "# if for no other reason. # Very good. But note that our nested panel is not quite working the way we want it to.  \n",
    "\n",
    "for n in range(1, 18):\n",
    "    counter = 1\n",
    "    TR       = 'TR' + str(n)\n",
    "    T        = 'T' + str(n)\n",
    "    Data[TR] = np.nan\n",
    "    for x in collections.OrderedDict.fromkeys(Data[T]):   #Set would be a great operation, but we lose order!\n",
    "        Data.loc[Data[T] == x, TR] = counter\n",
    "        counter = counter + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have a lot of words, but many are missing, we can pare things down to a reduced Swadesh list of those words that are most commonly occuring in the data. These are the following. Once we have it up and running we can take a quick look at a few words. One thing we should also add is some description of what these words are!\n",
    "\n",
    "At some point, we should add in word meanings. But let's get our pared-down list in place, and then take a look at how the **ASJP** renders them in `Ascii`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   name word1 word2   word77\n",
      "104                                SIWI   niS   Sik    adxax\n",
      "310               TASHELHIT/IDA_USEMLAL    nk    ky   tagunt\n",
      "3867  TAMAZIGHT_CENTRAL_ATLAS/AYT_NDHIR   n3k   S3g    azr7u\n",
      "207                             METMATA  n3CC  S3kk     azru\n",
      "206                             TUMZABT   n3S   n3C     adxa\n",
      "1002                     OUARGLA_BERBER  n3SS  S3kk    adXaX\n",
      "106               TARIFIT/BENI_IZNASSEN   n3C   S3k     azru\n",
      "601                      AHAGGAR_TUAREG   nak   kay   tahunt\n",
      "3172                           TAMASHEQ   nak   kay   t3hunt\n",
      "603                              ZENAGA  n37k   k3k  t37rg3t\n"
     ]
    }
   ],
   "source": [
    "words=['word1', 'word2', 'word3', 'word11', 'word12', 'word18', 'word19', 'word21', 'word22', 'word23', \n",
    "       'word25', 'word28', 'word30', 'word31', 'word34', 'word39', 'word40', 'word41', 'word43', 'word44', \n",
    "       'word47', 'word48', 'word51', 'word53', 'word54', 'word57', 'word58', 'word61', 'word66', 'word72', \n",
    "       'word74', 'word75', 'word77', 'word82', 'word85', 'word86', 'word92', 'word95', 'word96', 'word100']\n",
    "print(Data[ ['name', 'word1', 'word2', 'word77'] ][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1660    Ei\n",
      "Name: word1, dtype: object\n",
      "1660    yu\n",
      "Name: word2, dtype: object\n",
      "1660    wi\n",
      "Name: word3, dtype: object\n",
      "1660    w3n\n",
      "Name: word11, dtype: object\n",
      "1660    tu\n",
      "Name: word12, dtype: object\n",
      "1660    pers3n\n",
      "Name: word18, dtype: object\n",
      "1660    fiS\n",
      "Name: word19, dtype: object\n",
      "1660    dag\n",
      "Name: word21, dtype: object\n",
      "1660    laus\n",
      "Name: word22, dtype: object\n",
      "1660    tri\n",
      "Name: word23, dtype: object\n",
      "1660    lif\n",
      "Name: word25, dtype: object\n",
      "1660    skin\n",
      "Name: word28, dtype: object\n",
      "1660    bl3d\n",
      "Name: word30, dtype: object\n",
      "1660    bon\n",
      "Name: word31, dtype: object\n",
      "1660    horn\n",
      "Name: word34, dtype: object\n",
      "1660    ir\n",
      "Name: word39, dtype: object\n",
      "1660    Ei\n",
      "Name: word40, dtype: object\n",
      "1660    nos\n",
      "Name: word41, dtype: object\n",
      "1660    tu8\n",
      "Name: word43, dtype: object\n",
      "1660    t3N\n",
      "Name: word44, dtype: object\n",
      "1660    ni\n",
      "Name: word47, dtype: object\n",
      "1660    hEnd\n",
      "Name: word48, dtype: object\n",
      "1660    brest\n",
      "Name: word51, dtype: object\n",
      "1660    liv3r\n",
      "Name: word53, dtype: object\n",
      "1660    drink\n",
      "Name: word54, dtype: object\n",
      "1660    si\n",
      "Name: word57, dtype: object\n",
      "1660    hir\n",
      "Name: word58, dtype: object\n",
      "1660    dEi\n",
      "Name: word61, dtype: object\n",
      "1660    k3m\n",
      "Name: word66, dtype: object\n",
      "1660    s3n\n",
      "Name: word72, dtype: object\n",
      "1660    star\n",
      "Name: word74, dtype: object\n",
      "1660    wat3r\n",
      "Name: word75, dtype: object\n",
      "1660    ston\n",
      "Name: word77, dtype: object\n",
      "1660    fEir\n",
      "Name: word82, dtype: object\n",
      "1660    pE8\n",
      "Name: word85, dtype: object\n",
      "1660    maunt3n\n",
      "Name: word86, dtype: object\n",
      "1660    nEit\n",
      "Name: word92, dtype: object\n",
      "1660    ful\n",
      "Name: word95, dtype: object\n",
      "1660    nu\n",
      "Name: word96, dtype: object\n",
      "1660    nem\n",
      "Name: word100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# English versions:\n",
    "for word in words:\n",
    "    print(Data[word].loc[Data['name']=='ENGLISH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can intuit, anyways, that the words are: I, You, We, One, Two, Person (man), Fish, Dog, Louse, Tree, Leaf, Skin, Blood, Bone, Horn, Ear, Eye, Nose, Tooth, Tongue, Knee, Hand, Breast, Liver, Drink, See, Hear, Die, Come, Sun, Star, Water, Stone, Fire, Path, Mountain, Night, Full, New, Name. If one is interested, one can read about how and why these words are chosen from the [Swadesh list Wikipedia entry](https://en.wikipedia.org/wiki/Swadesh_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dogolpolsky classes\n",
    "\n",
    "The first actual function we will make reduces words into so-called Dogopolsky classes. These classes, while simple and inexact, have a nice attribute from the perspective of comparative linguistics: a constant number of states. If one relies on expert cognancy judgements, as is commonly done in historical linguistics, we have more classes for larger linguistic stocks. We avoid this by using a constant set of 10 states for languages. Some research suggests this isn't a terrible first approximation. \n",
    "\n",
    "We have two functions in the `PyIETools` module that accomplish this: `worddolg` which takes a word and returns its Dogolopolsky class, and `worddolgs`, which does the same for a list of words. We apply these functions and get back numeric classes in the following block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the function, we can convert our list into an array of Dogopolsky classes. We also print out a sample of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  2. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "Words = np.asarray(Data[words])\n",
    "W = PyIETools.worddolgs(Words[:][0])\n",
    "\n",
    "for i in range(1, len(Words)):\n",
    "    reduction = PyIETools.worddolgs(Words[:][i])\n",
    "    W = np.vstack((W,reduction))\n",
    "print(W[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The print statement shows what we have - an array of numbers for each society/language telling us in which class each word on our list belongs to. \n",
    "\n",
    "\n",
    "In estimation, we need to have these in a dummy variable form. The function `charnum_to_dummies` translates the numbers from the above work into a string of 10 dummies. Each word then becomes amenable to Markov chain analysis. Class 3, for example,  becomes\n",
    "`[0 0 1 0 0 0 0 0 0 0]`. Felsenstein notes that this can easily handle missing words: we just use a state indicator vector with all ones in it. `[1 1 1 1 1 1 1 1 1 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word10_p', 'word10_t', 'word10_s', 'word10_c', 'word10_m']\n",
      "104     0\n",
      "310     0\n",
      "3867    0\n",
      "207     0\n",
      "206     0\n",
      "1002    0\n",
      "106     0\n",
      "601     0\n",
      "3172    0\n",
      "603     0\n",
      "Name: word10_p, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "States = PyIETools.charnum_to_dummies(W)\n",
    "\n",
    "# We now have a bunch of dummies, but let's make these into variables so we can plop them back into the data frame:\n",
    "\n",
    "states = []\n",
    "dim1   = int(np.shape(States)[1]/10)\n",
    "DogList= ['p', 't', 's', 'c', 'm', 'N', 'l', 'w', 'y', 'i']\n",
    "for i in range(0, dim1):\n",
    "    for j in range(0, 10):\n",
    "        states.append(words[i] + str(i) + '_' + DogList[j])\n",
    "\n",
    "# Print out the results so we can see how they look as variables        \n",
    "\n",
    "print(states[0:5])\n",
    "\n",
    "# Pretty fancy...now, assign to a part of our dataframe\n",
    "\n",
    "for i in range(0, int(dim1*10)):\n",
    "    Data[ states[i] ] = States[:, i]\n",
    "\n",
    "# Print out result for verification:\n",
    "\n",
    "print(Data['word10_p'][0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A small sample - note how missings have been handled with a row of ones. \n",
    "\n",
    "States[:5,0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prior information on language splits\n",
    "\n",
    "The next thing to be done is to read in all of our prior information on splits. This is fundamental in manipulating trees, as it is what one must use to calibrate the tree. \n",
    "\n",
    "We have gathered this information from various sources which we should indicate, including Ackerman, the ASJP website, and also some additional sources. In fact, what we need to do is write in another column, so we keep track of where the sources are! Of course, the methods we use can easily allow for additional prior information on linguistic splits. \n",
    "\n",
    "All of our information is expressed in terms of bilateral splits. We read this in as a separate data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Splits=pd.read_csv('IEData\\\\PriorSplitInfo.csv',names=('phylum','language1','language2','years','sdyears'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      phylum      language1                language2  years  sdyears\n",
      "0  AfroAsia        ETHIOPIC                     GEEZ   2450       10\n",
      "1  AfroAsia        ETHIOPIC                    TIGRE   2450       10\n",
      "2  AfroAsia        ETHIOPIC                  ARGOBRA   2450       10\n",
      "3  AfroAsia         MALTESE  TUNISIAN_ARABIC_MAGHRIB    910       10\n",
      "4  AfroAsia   EASTERN_OROMO              MECHA_OROMO   2500       10\n"
     ]
    }
   ],
   "source": [
    "print(Splits[0:5])            # A small sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The other source of prior information for calibration of linguistic trees is prior depth. We haven't worked nearly hard enough on setting these up, being content merely to figure out how to incorporate the information for the time being. We have to go through this and pin down some information that is a bit more exact for the approximate depth of each language stock (in terms of millenia from the present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       phylum  min  max\n",
      "0   AfroAsia     9   12\n",
      "1     Altaic     6    8\n",
      "2     Amerind    9   11\n",
      "3     Austric    8   12\n",
      "4     Austral    9   12\n",
      "5   Burushask    3    6\n",
      "6   Caucasian    4    7\n",
      "7     Chukchi    3    6\n",
      "8    ElamoDra    4    6\n",
      "9    EskimoAl    4    5\n",
      "10   IndoHitt    7    9\n",
      "11    IndoPac    8   10\n",
      "12       Khet    3    5\n",
      "13    Khoisan   10   12\n",
      "14     NaDene    6    8\n",
      "15  NigerKord   10   12\n",
      "16  NiloSahar   10   12\n",
      "17  SinoTibet    6    8\n",
      "18  UralicYuk    5    7\n"
     ]
    }
   ],
   "source": [
    "Depths=pd.read_csv('IEData\\\\PriorDepth.csv',names=('phylum','min','max'))\n",
    "print(Depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final thing to do, we need to organize our material on expiration dates of certain languages. Some of this information is in the original data, while some of it is in a free-standing file, so we want to merge this information together and into our original data file. \n",
    "\n",
    "Here goes, while reading from the `DeathMods.csv` file which describes modifications to moribund languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge in information on deaths and then sort. \n",
    "DeathDates = pd.read_csv('IEData\\\\DeathMods.csv', names=('language', 'ex_date', 'ex_date_sd'))\n",
    "DeathDates.rename(columns = {'language': 'name'}, inplace=True)\n",
    "Data.set_index('name')\n",
    "Data['ex_date_new']    = pd.Series(DeathDates['ex_date'], index=DeathDates.name)\n",
    "Data['ex_date_sd_new'] = pd.Series(DeathDates['ex_date_sd'], index=DeathDates.name)\n",
    "\n",
    "# Now, replace ex_date with ex_date_new if ex_date_new is not missing...\n",
    "\n",
    "Data.loc[Data['ex_date_new']<10000 , \"ex_date\"  ]    = Data['ex_date_new']\n",
    "Data.loc[Data['ex_date_sd_new']<10000, \"ex_date_sd\"] = Data['ex_date_sd_new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling\n",
    "\n",
    "As a last step, we will pickle the data for ease of access in our next workbook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data.to_pickle('IEData\\\\MasterData.pkl')\n",
    "Splits.to_pickle('IEData\\\\Splits.pkl')\n",
    "Depths.to_pickle('IEData\\\\\\Depths.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
